{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "da3e41f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from geopy.distance import geodesic\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "\n",
    "import folium\n",
    "from folium.raster_layers import ImageOverlay\n",
    "from folium import plugins\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime,timezone,timedelta\n",
    "import pytz #time zone data\n",
    "\n",
    "import csv\n",
    "import exifread\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#commented all the #print functions because the api not working in aws deployment when we have #print functions\n",
    "\n",
    "#!pip install --upgrade requests # to overcome the ssl error in downloading\n",
    "\n",
    "#!pip install mysql-connector-python\n",
    "import mysql.connector\n",
    "#!pip install sqlalchemy\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "from download_weather_data import create_weather_dataset,temporal_processing_time_parallel\n",
    "\n",
    "#PDF_PI_FILE_PATH = \"./results/csv/grid_pdf_pi.csv\"\n",
    "#HIVE_DETAILS_FILE_PATH = \"./data/csv/hive_detailss.csv\"\n",
    "#WEATHER_DESCRIPTION_FILE_PATH = \"./data/csv/weather_description_map.csv\"\n",
    "#FINAL_WEATHER_DATA_FILE_PATH = \"./results/csv/final_weather_data.csv\"\n",
    "SPATIAL_MAP_SAVE_PATH = \"./results/maps/spatial_map.html\"\n",
    "FINAL_MAP_SAVE_PATH = \"./results/maps/final_map.html\"\n",
    "\n",
    "\n",
    "HIVE_DETAILS_QUERY = \"SELECT * FROM hive_details\"\n",
    "WEATHER_DESCRIPTION_QUERY = \"SELECT * FROM weather_description_map\"\n",
    "PDF_PI_FILE_QUERY = \"SELECT * FROM grid_pdf_pi\"\n",
    "FINAL_WEATHER_DATA_FILE_QUERY = \"SELECT * FROM final_weather_data\"\n",
    "\n",
    "PDF_PI_TABLE = \"grid_pdf_pi\"\n",
    "FINAL_WEATHER_TABLE = \"final_weather_data\"\n",
    "#MYSQL_CREDENTIALS = {\"host\":\"127.0.0.1\", \"user\":\"dilshan\", \"password\":\"1234\", \"database\":\"broodbox\", \"port\":3306}\n",
    "MYSQL_CREDENTIALS = {\"host\":\"127.0.0.1\", \"user\":\"root\", \"password\":\"\", \"database\":\"broodbox\", \"port\":3306}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617023e1",
   "metadata": {},
   "source": [
    "## Database Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1a029761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_mesql(query,credentials=MYSQL_CREDENTIALS):\n",
    "    \n",
    "    \"\"\"this function uses to read mysql table and returns it as a dataframe\"\"\"\n",
    "    # Connect to the MySQL server\n",
    "    connection = mysql.connector.connect(\n",
    "        host=credentials[\"host\"],\n",
    "        user=credentials[\"user\"],\n",
    "        password=credentials[\"password\"],\n",
    "        database=credentials[\"database\"]\n",
    "    )\n",
    "    \n",
    "    # Create a cursor object to interact with the database\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    # Execute the query\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch all rows from the result set\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    # Convert the data to a Pandas DataFrame\n",
    "    column_names = [i[0] for i in cursor.description]\n",
    "    df = pd.DataFrame(rows, columns=column_names)\n",
    "    \n",
    "    # Close the cursor and connection\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_mysql_table(dataset, table_name, credentials=MYSQL_CREDENTIALS):\n",
    "    \n",
    "    \"this function creates a table in mysql database using pandas dataframe\"\n",
    "    \n",
    "    engine = create_engine(f'mysql+mysqlconnector://{credentials[\"user\"]}:{credentials[\"password\"]}@{credentials[\"host\"]}:{credentials[\"port\"]}/{credentials[\"database\"]}', connect_args={\"connect_timeout\": 28800})\n",
    "\n",
    "    dataset.to_sql(table_name, con=engine, if_exists='replace', index=False)\n",
    "    \n",
    "    engine.dispose()\n",
    "    \n",
    "    \n",
    "def table_exist_mysql_database(table_name, credentials=MYSQL_CREDENTIALS):\n",
    "    \n",
    "    \"this function returns true if the given tables exists in the mysql database , other wise it rerurns false\"\n",
    "    # Create a MySQL connection\n",
    "    engine = create_engine(f'mysql+mysqlconnector://{credentials[\"user\"]}:{credentials[\"password\"]}@{credentials[\"host\"]}:{credentials[\"port\"]}/{credentials[\"database\"]}')\n",
    "\n",
    "    # Check if the dataset exists\n",
    "    inspector = inspect(engine)\n",
    "    tables = inspector.get_table_names()\n",
    "\n",
    "    # Print the result\n",
    "    if table_name in tables:\n",
    "        exist = True\n",
    "    else:\n",
    "        exist = False\n",
    "\n",
    "    # Close the connection (optional)\n",
    "    engine.dispose()\n",
    "\n",
    "    return exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faccabf0",
   "metadata": {},
   "source": [
    "## Spatial Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b6e057d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffrence_between_tow_points(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    \"\"\"This funtion finds the distance between two locations in Km when the longitudes and latitudes of the two points are given\"\"\"\n",
    "    \n",
    "    R = 6371 # radius of the eatch in kilo meters \n",
    "    lon1_rad = math.radians(lon1) # convert degrees to radians\n",
    "    lon2_rad = math.radians(lon2)    \n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lat2_rad = math.radians(lat2)   \n",
    "    \n",
    "    del_lon = lon2_rad - lon1_rad\n",
    "    del_lat = lat2_rad - lat1_rad\n",
    "    \n",
    "    a = (math.sin(del_lat/2))**2 + math.cos(lat1_rad)*math.cos(lat2_rad)*((math.sin(del_lon/2))**2)\n",
    "    c = 2*math.atan2(math.sqrt(a),math.sqrt(1-a))\n",
    "    d = R*c\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "def distence_probability(dist):\n",
    "    \n",
    "    \"\"\"Takes one distencs and convert it to probability\"\"\"\n",
    "    \n",
    "    # Distence probability function\n",
    "    d1 = (np.exp(dist/100))/15.6\n",
    "    d2 = 1.63/d1 \n",
    "    prob = np.where(dist<275,d1,np.where(dist<=325,1,np.where(dist<=500,d2,0)))\n",
    "         \n",
    "    return prob\n",
    "\n",
    "\n",
    "def distance_matrix(latitudes,longitudes,hive_details_dataset):\n",
    "    \n",
    "    \"\"\"this function finds distence between each grid point and hive location\"\"\" \n",
    "    \n",
    "    grid_point_latitudes = latitudes\n",
    "    grid_point_longitudes = longitudes\n",
    "\n",
    "    hive_point_latitudes = np.array(hive_details_dataset['latitude'])\n",
    "    hive_point_longitudes = np.array(hive_details_dataset['longitude']) \n",
    "    \n",
    "    distance_arr = []\n",
    "\n",
    "    for i in range(len(grid_point_latitudes)):\n",
    "\n",
    "        point1 = (grid_point_latitudes[i],grid_point_longitudes[i])\n",
    "\n",
    "        distance_vec = []\n",
    "\n",
    "        for j in range(len(hive_point_latitudes)):\n",
    "\n",
    "            point2 = (hive_point_latitudes[j],hive_point_longitudes[j])\n",
    "\n",
    "            distance = geodesic(point1, point2).meters\n",
    "            distance_vec.append(distance)\n",
    "\n",
    "        distance_arr.append(distance_vec)\n",
    "        \n",
    "    return np.array(distance_arr)\n",
    "\n",
    "\n",
    "def probability_matrix(distance_matrix,hive_details_dataset):\n",
    "    \n",
    "    \"\"\"This function convert the distences to probabilities\"\"\"\n",
    "    \n",
    "    total_frames = np.array(hive_details_dataset['total_active_frames'])\n",
    "    \n",
    "    prob_dist = [] # this is 2d vector containing all the probabilities of points form hives (3960*260)\n",
    "\n",
    "    for i in range(distance_matrix.shape[0]):\n",
    "\n",
    "        prob_dist_vec = [] # probabilities containing each grid point in a row (len 260)\n",
    "\n",
    "        for j in range(distance_matrix.shape[1]):\n",
    "\n",
    "            point_dist = distance_matrix[i][j] # distance from hive\n",
    "            prob = distence_probability(point_dist)\n",
    "\n",
    "            # append the (probability*total frames) crosponding distance range\n",
    "            prob_dist_vec.append(prob*total_frames[j])\n",
    "\n",
    "\n",
    "        prob_dist.append(prob_dist_vec) \n",
    "\n",
    "    prob_dist_arr = np.array(prob_dist)\n",
    "    \n",
    "    return prob_dist_arr\n",
    "\n",
    "\n",
    "def convert_one_probability(prob_dist_arr):\n",
    "    \n",
    "    \"\"\"This function takes matrics of probabilities and gives the sum of each raw them\"\"\"\n",
    "    # get the sum of each rows in the probability metrix\n",
    "    sum_prob_vec = [] # get the sum of all raws \n",
    "\n",
    "    for i in range(prob_dist_arr.shape[0]):\n",
    "\n",
    "        distance_row = prob_dist_arr[i]\n",
    "\n",
    "        sum_distance_row = np.sum(distance_row)\n",
    "\n",
    "        sum_prob_vec.append(sum_distance_row)\n",
    "\n",
    "    sum_prob_arr = np.array(sum_prob_vec) \n",
    "\n",
    "    norm_sum_prob_arr = (sum_prob_arr - np.min(sum_prob_arr))/(np.max(sum_prob_arr)-np.min(sum_prob_arr)) #  normalized sum_prob_arr using min max formula\n",
    "\n",
    "    return norm_sum_prob_arr\n",
    "\n",
    "\n",
    "def spatial_probability_dataset(lat,long):\n",
    "    \n",
    "    \"\"\"This is the finla function. this function call all the above funcions to make the sptial probabilities\"\"\"\n",
    "    \n",
    "    hive_details_dataset = read_data_from_mesql(HIVE_DETAILS_QUERY)\n",
    "    \n",
    "    distances = distance_matrix(lat,long,hive_details_dataset)\n",
    "    porbabilities = probability_matrix(distances,hive_details_dataset)  \n",
    "    norm_sum_prob_arr = convert_one_probability(porbabilities)\n",
    "    \n",
    "    data = {'id':np.arange(1,len(norm_sum_prob_arr)+1,1), 'longitude':long,'latitude':lat, 'spatial_prob':norm_sum_prob_arr}\n",
    "    dataset = pd.DataFrame(data)\n",
    "    create_mysql_table(dataset, \"grid_pdf_pi\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f35b8",
   "metadata": {},
   "source": [
    "## Weather Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d8ceb5",
   "metadata": {},
   "source": [
    "#### Weather Probability functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "676f68cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tempreture_probability(tempreture):\n",
    "\n",
    "    t1= 0.141*np.exp(tempreture/10) - 0.1\n",
    "    t2 = (3.4/t1)-0.42\n",
    "    prob = np.where(tempreture<0,0,np.where(tempreture<=20,t1,np.where(tempreture<=30,1,np.where(tempreture<=40,t2,0))))\n",
    "    \n",
    "    return prob\n",
    "\n",
    "def humidity_probability(humidity):\n",
    "    \n",
    "    h1 = 0.0322*np.exp(humidity/10)\n",
    "    h2 = 2.7/h1 \n",
    "    prob = np.where(humidity<35,h1,np.where(humidity<=45,1,h2))\n",
    "    \n",
    "    return prob\n",
    "\n",
    "def wind_probability(speed):\n",
    "                         \n",
    "    w1 = 3*np.exp(-speed/10) - 0.15\n",
    "    prob = np.where(speed<=10,1,np.where(speed>=30,0,w1))\n",
    "    return prob\n",
    "\n",
    "def hour_probability(data_set):\n",
    "    \n",
    "    \"\"\"This function returns 1 if the sun in sky, otherwise gives 0\"\"\"\n",
    "    \n",
    "    hour_prob = []\n",
    "    \n",
    "    for i in range(data_set.shape[0]):\n",
    "        \n",
    "        #get times as strings\n",
    "        sunrise_str = str(data_set[\"sunrise\"][i]).split()[1]\n",
    "        sunset_str = str(data_set[\"sunset\"][i]).split()[1]\n",
    "        curr_time_str = str(data_set[\"time\"][i])[:8]\n",
    "        \n",
    "        #get time strings as time objects\n",
    "        sunrise = datetime.strptime(sunrise_str, '%H:%M:%S').time()\n",
    "        sunset = datetime.strptime(sunset_str, '%H:%M:%S').time()\n",
    "        curr_time = datetime.strptime(curr_time_str, '%H:%M:%S').time()\n",
    "        \n",
    "        #checks the current time and sunset and sunrise\n",
    "        if(sunrise<=curr_time<=sunset):\n",
    "            hour_prob.append(1.0)\n",
    "        else:\n",
    "            hour_prob.append(0.0)\n",
    "            \n",
    "    return hour_prob\n",
    "\n",
    "def should_spatial_probability_call(file_path):\n",
    "    \"\"\"this function returns true if the hive_details file not exist or it was updated withing 5 mins otherwise it returns false\"\"\"\n",
    "    update = False    \n",
    "    # Check if the file not exists updated before 300 seconnds\n",
    "    if not os.path.exists(file_path):\n",
    "        update = True\n",
    "        return update\n",
    "    \n",
    "    current_time = datetime.now().timestamp()\n",
    "    # Get file metadata covert it to seconds using timestamp()\n",
    "    modification_time = datetime.fromtimestamp(os.path.getmtime(file_path)).timestamp()\n",
    "    \n",
    "    # Check if the file updated before 300 seconnds\n",
    "    if (int(current_time -modification_time) <= 300):\n",
    "        update = True\n",
    "   \n",
    "    return update\n",
    "                         \n",
    "def final_probability(data_set,lat,long):\n",
    "    \n",
    "    table_exist = table_exist_mysql_database(PDF_PI_TABLE)\n",
    "    #load spatial porbability data set \n",
    "    if table_exist==False:\n",
    "        spatial_probability_dataset(lat,long)\n",
    "        spatial_prob_data = read_data_from_mesql(PDF_PI_FILE_QUERY)\n",
    "    else:\n",
    "        spatial_prob_data = read_data_from_mesql(PDF_PI_FILE_QUERY)\n",
    "      \n",
    "    #load weather description porbability data set\n",
    "    weather_desc_data = read_data_from_mesql(WEATHER_DESCRIPTION_QUERY)\n",
    "    # genarate weather probability using mean ratings\n",
    "    weather_desc_data[\"probability\"] = (weather_desc_data[\"mean_ratings\"]-1)/(10-1)\n",
    "\n",
    "    data_set[\"weather_condition_prob\"] = list(weather_desc_data[weather_desc_data[\"weather_id\"]==list(data_set[\"weather_id\"])[0]][\"probability\"])[0]\n",
    "\n",
    "    \n",
    "    #hour probability\n",
    "    hour_prob_arr = hour_probability(data_set)\n",
    "    data_set[\"hour_prob\"] = hour_prob_arr\n",
    "    \n",
    "    data_set[\"tempreture_prob\"] = data_set[\"tempreture\"].apply(tempreture_probability)\n",
    "    data_set[\"humidity_prob\"] = data_set[\"humidity\"].apply(humidity_probability)\n",
    "    data_set[\"wind_prob\"] = data_set[\"wind_speed\"].apply(wind_probability)\n",
    "                                    \n",
    "                                    \n",
    "    prob = np.array(data_set[\"tempreture_prob\"]*data_set[\"humidity_prob\"]*data_set[\"wind_prob\"]* data_set[\"weather_condition_prob\"]*data_set[\"hour_prob\"])\n",
    "    data_set[\"weather_prob\"] = prob\n",
    "    \n",
    "    final_data_set = pd.merge(data_set,spatial_prob_data, on='id')\n",
    "    final_data_set[\"final_prob\"] = final_data_set[\"weather_prob\"]*final_data_set[\"spatial_prob\"]\n",
    "    final_data_set.drop(columns=[\"longitude_y\",\"latitude_y\"], axis=1, inplace = True)\n",
    "    final_data_set.rename(columns = {'longitude_x':'longitude', 'latitude_x':'latitude'}, inplace = True)\n",
    "    \n",
    "    \n",
    "    return final_data_set\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe52eaa",
   "metadata": {},
   "source": [
    "#### Weather data donwload functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fadcb480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_weather_data_raw(latitudes,longitudes,cols,speed_up=4):\n",
    "    \n",
    "    # this function extract the weather data from api when provide the lat and long arrays (each raw of latitude)\n",
    "    # the speed_up factor  determines that how may weather data values paeted by previous copied value, here it is pasted 3 values (4-1=3) by previous copied value.\n",
    "    # here cols means number of points in a raw\n",
    "    # create a data frame\n",
    "    grid_point_Weather_data = pd.DataFrame(columns=[\"time\",\"longitude\", \"latitude\",\"tempreture\", \"humidity\",\"wind_speed\",\"weather_id\", \"weather_id_group\", \"weather_id_description\", \"sunrise\", \"sunset\"])\n",
    "    srt_time  = datetime.now()\n",
    "    piangil_timezone = pytz.timezone('Australia/Sydney')\n",
    "\n",
    "    for i in range(int(cols/speed_up)): # contralls the amount of the data\n",
    "\n",
    "        srt_time_point  = datetime.now()\n",
    "        #get the lat long coordinates\n",
    "        lat = latitudes[speed_up*i] \n",
    "        long = longitudes[speed_up*i]\n",
    "\n",
    "        #API url\n",
    "        url = \"https://api.openweathermap.org/data/2.5/weather?lat={}&lon={}&appid=8ee842d65cf08ec205365865e3d53348&units=metric\".format(lat,long)\n",
    "        ##print(srt_time_point)\n",
    "        ##print(url)\n",
    "\n",
    "        piangil_time = datetime.now(piangil_timezone) #get time in Australia for data set\n",
    "        \n",
    "        #get data form API as json data (here wile loop is used to prevent to SSL erro failers)\n",
    "        loop_though = True\n",
    "        while loop_though:  \n",
    "            try:\n",
    "                res = requests.get(url)\n",
    "                loop_though = False\n",
    "            except:\n",
    "                pass\n",
    "        data = res.json()\n",
    "\n",
    "        # create the data list that we want from the json data \n",
    "        data_vec = [piangil_time,long, lat, data[\"main\"][\"temp\"], data[\"main\"][\"humidity\"], data[\"wind\"][\"speed\"], data[\"weather\"][0][\"id\"], data[\"weather\"][0][\"main\"], data[\"weather\"][0][\"description\"], unix_to_aus(data[\"sys\"][\"sunrise\"]), unix_to_aus(data[\"sys\"][\"sunset\"])]\n",
    "        data_vec_1 = [piangil_time,longitudes[speed_up*i+1], latitudes[speed_up*i+1], data[\"main\"][\"temp\"], data[\"main\"][\"humidity\"], data[\"wind\"][\"speed\"], data[\"weather\"][0][\"id\"], data[\"weather\"][0][\"main\"], data[\"weather\"][0][\"description\"], unix_to_aus(data[\"sys\"][\"sunrise\"]), unix_to_aus(data[\"sys\"][\"sunset\"])]\n",
    "        data_vec_2 = [piangil_time,longitudes[speed_up*i+2], latitudes[speed_up*i+2], data[\"main\"][\"temp\"], data[\"main\"][\"humidity\"], data[\"wind\"][\"speed\"], data[\"weather\"][0][\"id\"], data[\"weather\"][0][\"main\"], data[\"weather\"][0][\"description\"], unix_to_aus(data[\"sys\"][\"sunrise\"]), unix_to_aus(data[\"sys\"][\"sunset\"])]\n",
    "        data_vec_3 = [piangil_time,longitudes[speed_up*i+3], latitudes[speed_up*i+3], data[\"main\"][\"temp\"], data[\"main\"][\"humidity\"], data[\"wind\"][\"speed\"], data[\"weather\"][0][\"id\"], data[\"weather\"][0][\"main\"], data[\"weather\"][0][\"description\"], unix_to_aus(data[\"sys\"][\"sunrise\"]), unix_to_aus(data[\"sys\"][\"sunset\"])]\n",
    "\n",
    "\n",
    "        #update the data frame\n",
    "        grid_point_Weather_data.loc[speed_up*i] = data_vec\n",
    "        grid_point_Weather_data.loc[speed_up*i+1] = data_vec_1\n",
    "        grid_point_Weather_data.loc[speed_up*i+2] = data_vec_2\n",
    "        grid_point_Weather_data.loc[speed_up*i+3] = data_vec_3\n",
    "\n",
    "        # if the longitudes arr length (or raw length of the map points) can not divide by speed_up then remaining point in the columns should be filled previous values\n",
    "        if(i%((int(cols/speed_up))-1)==0) and (cols%speed_up !=0) and (i!=0):\n",
    "            num = cols%speed_up\n",
    "            for j in range(num):\n",
    "                data_vec_j = [piangil_time,longitudes[speed_up*i+3+(j+1)], latitudes[speed_up*i+3+(j+1)], data[\"main\"][\"temp\"], data[\"main\"][\"humidity\"], data[\"wind\"][\"speed\"], data[\"weather\"][0][\"id\"], data[\"weather\"][0][\"main\"], data[\"weather\"][0][\"description\"], unix_to_aus(data[\"sys\"][\"sunrise\"]), unix_to_aus(data[\"sys\"][\"sunset\"])]\n",
    "                grid_point_Weather_data.loc[speed_up*i+3+(j+1)] = data_vec_j\n",
    "                #print(f\"this is done when step is equals to {i+1}\")\n",
    "\n",
    "\n",
    "        time.sleep(0.175)\n",
    "        end_time_point  = datetime.now()\n",
    "        #print(f\"step {i+1} is completed! and taken {end_time_point-srt_time_point} time to complete\")\n",
    "\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    total_execution_time = end_time-srt_time\n",
    "    #print(f\"the programe take: {total_execution_time} to complete\")\n",
    "    \n",
    "          \n",
    "    return grid_point_Weather_data\n",
    "\n",
    "\n",
    "def download_weather_data_old(latitudes,longitudes,cols,raws):\n",
    "    \n",
    "    grid_point_Weather_data = pd.DataFrame(columns=[\"time\",\"longitude\", \"latitude\",\"tempreture\", \"humidity\",\"wind_speed\",\"weather_id\", \"weather_id_group\", \"weather_id_description\", \"sunrise\", \"sunset\"])\n",
    "    \n",
    "    for i in range(raws):\n",
    "        \n",
    "        # selecting each raw of latitude and longitude arrays\n",
    "        lat_arr = latitudes[i*cols:(i+1)*cols]\n",
    "        long_arr = longitudes[i*cols:(i+1)*cols] \n",
    "        \n",
    "        # get weather data for each raw of latitudes and longitudes\n",
    "        first_batch_data = download_weather_data_raw(lat_arr,long_arr,cols)\n",
    "        \n",
    "        # combine the pandas dataframe with previoues one\n",
    "        grid_point_Weather_data = pd.concat([grid_point_Weather_data,first_batch_data], axis=0, ignore_index=True)\n",
    "        #print(f\"complete the {i+1} raw data download\")\n",
    "        #print(\"==================\")\n",
    "        #print(\"==================\")\n",
    "    \n",
    "    # set the Id column and charge the raw order\n",
    "    grid_point_Weather_data[\"id\"] = [j+1 for j in range(cols*raws)]\n",
    "    grid_point_Weather_data = grid_point_Weather_data[[\"id\",\"time\",\"longitude\", \"latitude\",\"tempreture\", \"humidity\",\"wind_speed\",\"weather_id\", \"weather_id_group\", \"weather_id_description\", \"sunrise\", \"sunset\"]]\n",
    "    \n",
    "    return grid_point_Weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b7084",
   "metadata": {},
   "source": [
    "#### Weather data preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3d888845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unix_to_aus(time):\n",
    "    \n",
    "    \"\"\"this function convert UNIX date time to Austrelia date time and output will be string. This function is called\n",
    "    inside the download_weather_data_raw function \"\"\"\n",
    "    \n",
    "    time_int = int(time) #get integer value\n",
    "    \n",
    "    time_zone = timezone(timedelta(seconds=36000)) # time zone of Austrelia \n",
    "    \n",
    "    aus_time = datetime.fromtimestamp(time_int, tz = time_zone).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    #aus_time = datetime.fromtimestamp(time_int, tz = time_zone)\n",
    "    \n",
    "    return aus_time\n",
    "\n",
    "\n",
    "def add_date_time(dataset):\n",
    "    \n",
    "    \"\"\"This function add a date column and time column for a given pandas dataframe using Sunrice column data\n",
    "     and Time column data.\"\"\"\n",
    "    \n",
    "    # create a date column as first column\n",
    "    date_column = dataset[\"sunrise\"].apply(lambda x: ((str(x)).split())[0])\n",
    "    dataset.insert(1, \"date\",date_column)\n",
    "\n",
    "    # update the Time column\n",
    "    dataset[\"time\"] = dataset[\"time\"].apply(lambda x: (str(x)).split()[1][:11])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e04d78",
   "metadata": {},
   "source": [
    "## Image taken locations plot functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8aa6efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_metadata_exif(image_path):\n",
    "    \"\"\"his function returns the meta data ofimages\"\"\"\n",
    "    try:\n",
    "        with open(image_path, 'rb') as img_file:\n",
    "            # Get Exif tags\n",
    "            tags = exifread.process_file(img_file)\n",
    "            return tags\n",
    "    except Exception as e:\n",
    "        ##print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def convert_lat_long(list_lat,list_long):\n",
    "    \"\"\"This function convert lat long meta deta to actual lat long coordinates\"\"\"\n",
    "    #decimal degrees = degrees + minutes / 60 + seconds / 3600\n",
    "    if list_lat == \"\" or list_long == \"\":\n",
    "        lat  = np.NaN\n",
    "        long = np.NaN\n",
    "    \n",
    "    else:\n",
    "        lat = -(list_lat[0] + list_lat[1]/60 + list_lat[2]/3600)\n",
    "        long = list_long[0] + list_long[1]/60 + list_long[2]/3600 \n",
    "    \n",
    "    return lat,long\n",
    "\n",
    "def image_taken_location_dataset(img_folder_path):\n",
    "    \"\"\"This function loop througth each image and extract meta data and find lat long and frame counts and make pandas table\"\"\"\n",
    "    \n",
    "    image_names = os.listdir(img_folder_path)\n",
    "    image_psths = [f\"./images/images/{img}\" for img in image_names]\n",
    "\n",
    "    frame_count_arr = []\n",
    "    lat_arr = []\n",
    "    long_arr = []\n",
    "\n",
    "    frame_key = \"Image ImageDescription\" \n",
    "    lat_key = \"GPS GPSLatitude\"\n",
    "    long_key = \"GPS GPSLongitude\"\n",
    "\n",
    "    for img_pth in image_psths:\n",
    "\n",
    "        result = read_image_metadata_exif(img_pth)\n",
    "\n",
    "        if frame_key in result:\n",
    "            frame_count = str(result[\"Image ImageDescription\"])\n",
    "        else:\n",
    "            frame_count = np.NaN\n",
    "\n",
    "        if lat_key in result:\n",
    "            lat_list = eval(str(result[\"GPS GPSLatitude\"]))\n",
    "        else:\n",
    "            lat_list = \"\"\n",
    "\n",
    "        if long_key in result:\n",
    "            long_list = eval(str(result[\"GPS GPSLongitude\"]))\n",
    "        else:\n",
    "            long_list = \"\"\n",
    "\n",
    "        lat,long = convert_lat_long(lat_list,long_list)\n",
    "        frame_count_arr.append(frame_count)\n",
    "        lat_arr.append(lat)\n",
    "        long_arr.append(long)\n",
    "\n",
    "    data = {\"lat\":lat_arr, \"long\":long_arr, \"frame_count\":frame_count_arr}\n",
    "    dataset = pd.DataFrame(data)\n",
    "    updated_dataset = dataset.dropna()\n",
    "    return updated_dataset\n",
    "\n",
    "\n",
    "def location_grid_frame_count(img_folder_path):\n",
    "    \"\"\"This function create lat long coordinate pairs for each location and preprocess the frame count of the pandas data frame and returns\"\"\"\n",
    "    \n",
    "    updated_dataset = image_taken_location_dataset(img_folder_path)\n",
    "    location = list(zip(updated_dataset[\"lat\"],updated_dataset[\"long\"]))\n",
    "    frame_count = updated_dataset[\"frame_count\"].values\n",
    "    frame_count[608] = '4/4,10/10' #this point's original value contains error for the map\n",
    "    \n",
    "    return location,frame_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af135e26",
   "metadata": {},
   "source": [
    "## User Input Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "48c4df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input_to_latlong_old():\n",
    "    \n",
    "    \"\"\"This funtion takes the user data  in form of latitudes and longitudes like this: max latitude,min latitude, min longitude, max longitude\n",
    "    and return the point grid varctors of given latitude and longitude boundaries\"\"\"\n",
    "\n",
    "    user_input = input(\"Enter the Lat Long codinates separated by a comma:\")\n",
    "\n",
    "    # get and evaluate the user inputs\n",
    "    try:\n",
    "        splited_input = user_input.split(\",\")\n",
    "\n",
    "        # user can only enter four numbers\n",
    "        if(len(splited_input)==4):\n",
    "            start_latitude = float(splited_input[0])\n",
    "            end_latitude = float(splited_input[1])\n",
    "\n",
    "            start_longitude = float(splited_input[2])\n",
    "            end_longitude = float(splited_input[3])\n",
    "\n",
    "            ##print(f\"Your start and end latitudes are:{[start_latitude,end_latitude]} and start and end longitudes are:{[start_longitude,end_longitude]}\")\n",
    "        else:\n",
    "            pass\n",
    "            ##print(\"Exceed or less number of inputes. Check the inputs again.\")\n",
    "\n",
    "\n",
    "    except (ValueError,IndexError):\n",
    "        pass\n",
    "        ##print(\"Error! Invalid input. Please enter valied input\")\n",
    "\n",
    "\n",
    "    # extract the data from user inputs    \n",
    "    start_lat = start_latitude\n",
    "    end_lat = end_latitude\n",
    "    start_long = start_longitude\n",
    "    end_long = end_longitude\n",
    "\n",
    "\n",
    "    separation_meters = 70\n",
    "    factor = 0.001 # for get points same as Qgis\n",
    "    separation_degrees = separation_meters/111000  #One degree of latitude is approximately 111 kilometers\n",
    "    num_of_points_lat = round(((abs(end_lat - start_lat)/factor) + 1))\n",
    "    num_of_points_long = round(((abs(end_long - start_long)/factor) + 1))\n",
    "\n",
    "\n",
    "\n",
    "    latitudes_arr = np.linspace(start_lat,end_lat,num_of_points_lat)\n",
    "    longitudes_arr = np.linspace(start_long,end_long,num_of_points_long)\n",
    "\n",
    "    # create grid points \n",
    "    point_grid = [(lat,long) for lat in latitudes_arr for long in longitudes_arr]\n",
    "    latitudes = np.array([point_grid[i][0] for i in range(len(point_grid))])\n",
    "    longitudes = np.array([point_grid[i][1] for i in range(len(point_grid))])\n",
    "    \n",
    "    # here longitudes_arr array containing the number of points in x direction (columns)\n",
    "    # here latitudes_arr array containing the number of points in ydirection (raws)\n",
    "    \n",
    "    return latitudes,longitudes,len(longitudes_arr),len(latitudes_arr)\n",
    "\n",
    "\n",
    "def user_input_boundaries_to_latlong(lat_boundaries,long_boundaries):\n",
    "    \n",
    "    \"\"\"This is the moest updated user input function. This funtion takes the user data in the form of lat long boundry arays\n",
    "    and then returns the point grid varctors of given latitude and longitude boundaries\"\"\"\n",
    "    \n",
    "    box_lat_list, box_long_list = find_box(lat_boundaries,long_boundaries)\n",
    "    \n",
    "    # extract the data from user inputs    \n",
    "    start_lat = max(box_lat_list)\n",
    "    end_lat = min(box_lat_list)\n",
    "    start_long = min(box_long_list)\n",
    "    end_long = max(box_long_list)\n",
    "\n",
    "    separation_meters = 70\n",
    "    factor = 0.001 # for get points same as Qgis\n",
    "    separation_degrees = separation_meters/111000  #One degree of latitude is approximately 111 kilometers\n",
    "    num_of_points_lat = round(((abs(end_lat - start_lat)/factor) + 1))\n",
    "    num_of_points_long = round(((abs(end_long - start_long)/factor) + 1))\n",
    "\n",
    "\n",
    "\n",
    "    latitudes_arr = np.linspace(start_lat,end_lat,num_of_points_lat)\n",
    "    longitudes_arr = np.linspace(start_long,end_long,num_of_points_long)\n",
    "\n",
    "    # create grid points \n",
    "    point_grid = [(lat,long) for lat in latitudes_arr for long in longitudes_arr]\n",
    "    latitudes = np.array([point_grid[i][0] for i in range(len(point_grid))])\n",
    "    longitudes = np.array([point_grid[i][1] for i in range(len(point_grid))])\n",
    "    \n",
    "    # here longitudes_arr array containing the number of points in x direction (columns)\n",
    "    # here latitudes_arr array containing the number of points in ydirection (raws)\n",
    "    \n",
    "    return latitudes,longitudes,len(longitudes_arr),len(latitudes_arr)\n",
    "\n",
    "\n",
    "def api_to_latlong(lat_boundaries,long_boundaries):\n",
    "    \n",
    "    \"\"\"Thsi function takes  lat longs boundaries form the api and returns the point grid varctors of given latitude and longitude boundaries\"\"\" \n",
    "    start_lat,end_lat,start_long,end_long = max(lat_boundaries), min(lat_boundaries), min(long_boundaries), max(long_boundaries)\n",
    "    separation_meters = 70\n",
    "    factor = 0.001 # for get points same as Qgis\n",
    "    separation_degrees = separation_meters/111000  #One degree of latitude is approximately 111 kilometers\n",
    "    num_of_points_lat = round(((abs(end_lat - start_lat)/factor) + 1))\n",
    "    num_of_points_long = round(((abs(end_long - start_long)/factor) + 1))\n",
    "\n",
    "    latitudes_arr = np.linspace(start_lat,end_lat,num_of_points_lat)\n",
    "    longitudes_arr = np.linspace(start_long,end_long,num_of_points_long)\n",
    "\n",
    "    # create grid points \n",
    "    point_grid = [(lat,long) for lat in latitudes_arr for long in longitudes_arr]\n",
    "    latitudes = np.array([point_grid[i][0] for i in range(len(point_grid))])\n",
    "    longitudes = np.array([point_grid[i][1] for i in range(len(point_grid))])\n",
    "    \n",
    "    # here longitudes_arr array containing the number of points in x direction (columns)\n",
    "    # here latitudes_arr array containing the number of points in ydirection (raws)\n",
    "    \n",
    "    return latitudes,longitudes,len(longitudes_arr),len(latitudes_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5be123e",
   "metadata": {},
   "source": [
    "## User Output Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b360493",
   "metadata": {},
   "source": [
    "#### Masking functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b897ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_box(lat_boundaries,long_boundaries):\n",
    "    \n",
    "    \"\"\"This function takes user boundaries and then finds and plots the square coodinates for convers the entier farm with \n",
    "    user boundaries. using the outputs we can find the max min lat long coordinates\"\"\"\n",
    "    \n",
    "    # to make the enclosed boundry \n",
    "    lat_boundaries[-1] = lat_boundaries[0]\n",
    "    long_boundaries[-1] = long_boundaries[0]\n",
    "    \n",
    "    box_long_list = [min(long_boundaries),max(long_boundaries), max(long_boundaries), min(long_boundaries)]\n",
    "    box_lat_list = [min(lat_boundaries),min(lat_boundaries),  max(lat_boundaries), max(lat_boundaries)]\n",
    "    \n",
    "    #if need to show plots then uncomment\n",
    "    # for plot a complete square\n",
    "    box_long_list_plot = box_long_list.append(box_long_list[0])\n",
    "    box_lat_list_plot = box_lat_list.append(box_lat_list[1])\n",
    "    \n",
    "    #plot the user boundaries and box boundaries\n",
    "    plt.scatter(long_boundaries,lat_boundaries)\n",
    "    plt.scatter(box_long_list,box_lat_list)\n",
    "\n",
    "    plt.plot(long_boundaries,lat_boundaries, 'b')\n",
    "    plt.plot(box_long_list,box_lat_list,'r')\n",
    "    \n",
    "    return box_lat_list,box_long_list\n",
    "\n",
    "\n",
    "def make_mask_fromm_boundaries(lats,longs,lat_boundaries,long_boundaries):\n",
    "    \"\"\"This function takes lats longs form the data frame and user farm boundaries then creates a mask array using user enterd \n",
    "    boundaries \"\"\"\n",
    "\n",
    "    mask = []\n",
    "    boundaries = list(zip(long_boundaries,lat_boundaries)) # get each points as list\n",
    "    polygon = Polygon(boundaries) # create a polygon usign boundaries\n",
    "\n",
    "    for i in range(len(lats)):\n",
    "\n",
    "        long = longs[i]\n",
    "        lat = lats[i] \n",
    "        point = Point(long,lat)\n",
    "\n",
    "        # if the point inside the polygon\n",
    "        if(polygon.contains(point)):\n",
    "            mask.append(1)\n",
    "        else:\n",
    "            mask.append(0)\n",
    "            \n",
    "    return np.array(mask)\n",
    "\n",
    "\n",
    "def make_mask_dataset(lat_boundaries,long_boundaries,data_set):\n",
    "    \n",
    "    \"\"\"This function removes the data points that are not withing user boundaries. this is the function that we need to call to create final data set\n",
    "    when we are using the uer defined boundaries. above two masked functions used here\"\"\"\n",
    "    \n",
    "    box_lat_list, box_long_list = find_box(lat_boundaries,long_boundaries)\n",
    "    \n",
    "    lats = data_set[\"latitude\"].values\n",
    "    longs = data_set[\"longitude\"].values\n",
    "    # create a mask\n",
    "    mask = make_mask_fromm_boundaries(lats,longs,lat_boundaries,long_boundaries)\n",
    "\n",
    "    #selected the points withn the user boundry and update the table \n",
    "    data_set[\"mask\"] = mask\n",
    " \n",
    "    masked_data_set = data_set.iloc[data_set[data_set[\"mask\"]==1].index].drop([\"mask\"], axis=1)\n",
    "    #if need to show plots then uncomment\n",
    "    #plot the selected lat long points using mask\n",
    "    plt.scatter(masked_data_set[\"longitude\"],masked_data_set[\"latitude\"],2)\n",
    "    plt.title(\"The user defined boundaries and selected data points to show result\")\n",
    "   \n",
    "    return masked_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "597a958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_processing_time(lat_boundaries,long_boundaries):\n",
    "    \"\"\"this function returns approximation time to calculate spatial probabilities\"\"\"\n",
    "    latitudes,longitudes,cols, raws = user_input_boundaries_to_latlong(lat_boundaries,long_boundaries)\n",
    "    hive_details_dataset = read_data_from_mesql(HIVE_DETAILS_QUERY)\n",
    "    no_of_locations = len(hive_details_dataset)\n",
    "    num_of_points = no_of_locations*cols*raws\n",
    "\n",
    "    approx_time_mins = int((num_of_points/4000)/60) # 4000 a is experimental value\n",
    "    if approx_time_mins==0:\n",
    "        approx_time_mins=1\n",
    "        \n",
    "    return approx_time_mins\n",
    "\n",
    "def temporal_processing_time(lat_boundaries,long_boundaries,speed_up=4,api_speed=60):\n",
    "    \"\"\"this function returns approximation time to download the weather data\"\"\"\n",
    "    latitudes,longitudes,cols, raws = user_input_boundaries_to_latlong(lat_boundaries,long_boundaries)\n",
    "\n",
    "    num_of_points = cols*raws\n",
    "    download_points = round(num_of_points/speed_up)\n",
    "    approx_time_mins = round(download_points/api_speed)  \n",
    "    if approx_time_mins==0:\n",
    "        approx_time_mins=1\n",
    "        \n",
    "    return approx_time_mins\n",
    "\n",
    "def spatial_heatmap(lat,long,lat_boundaries, long_boundaries, image_path=\" \"):\n",
    "    \"\"\"This function returns the spatial heatmap\"\"\"  \n",
    "    #load spatial porbability data set\n",
    "    table_exist = table_exist_mysql_database(PDF_PI_TABLE)\n",
    "    #load spatial porbability data set \n",
    "    if table_exist==False:\n",
    "        spatial_probability_dataset(lat,long)\n",
    "        dataset = read_data_from_mesql(PDF_PI_FILE_QUERY)\n",
    "    else:\n",
    "        dataset = read_data_from_mesql(PDF_PI_FILE_QUERY)\n",
    "\n",
    "    #applying masking\n",
    "    dataset = make_mask_dataset(lat_boundaries,long_boundaries,dataset)\n",
    "    if dataset.empty == True:\n",
    "        m = np.NaN\n",
    "        return m\n",
    "         \n",
    "    longitudes = dataset[\"longitude\"]\n",
    "    latitudes = dataset[\"latitude\"]\n",
    "    probability = dataset[\"spatial_prob\"]\n",
    "\n",
    "    # heat map data ([lat,long,prob])\n",
    "    heatdata = [list(i) for i in list(zip(latitudes,longitudes,probability))]\n",
    "\n",
    "    # Create a base map with satellite tiles\n",
    "    m = folium.Map(location=[sum(latitudes)/len(latitudes), sum(longitudes)/len(longitudes)], \n",
    "                   zoom_start=14,\n",
    "                   max_zoom=15,\n",
    "                   tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
    "                   attr='Google Satellite',\n",
    "                   width='100%',\n",
    "                   height = '100%')\n",
    "    \n",
    "    # plot heatmap on the map\n",
    "    plugins.HeatMap(heatdata, radius=20, blur=20, min_opacity=0.0).add_to(m)\n",
    "    \n",
    "    \"\"\"# #print each point in the map\n",
    "    for point in point_grid:\n",
    "        folium.Marker(location=point,popup=str(point)).add_to(m)\"\"\"\n",
    "    \n",
    "    # add farm image on the mapp\n",
    "    try:\n",
    "        if(image_path != \" \"):\n",
    "\n",
    "            overlay =  ImageOverlay(\n",
    "                image_path,\n",
    "                bounds= [[min(latitudes),min(longitudes)],[max(latitudes),max(longitudes)]],\n",
    "                opacity = 0.5\n",
    "            )\n",
    "\n",
    "            overlay.add_to(m)\n",
    "    except:\n",
    "        pass\n",
    "        ##print(\"Please provide a valid image path\")\n",
    "\n",
    "    return m\n",
    "\n",
    "def temporal_heatmap(lat_boundaries, long_boundaries, dataset, image_path=\" \"):\n",
    "    \"\"\"This function returns the temporal heatmap\"\"\"\n",
    "    \n",
    "    #applying masking\n",
    "    dataset = make_mask_dataset(lat_boundaries,long_boundaries,dataset)\n",
    "    if dataset.empty == True:\n",
    "        m = np.NaN\n",
    "        return m\n",
    "    \n",
    "    longitudes = dataset[\"longitude\"]\n",
    "    latitudes = dataset[\"latitude\"]\n",
    "    probability = dataset[\"weather_prob\"]\n",
    "\n",
    "    # heat map data ([lat,long,prob])\n",
    "    heatdata = [list(i) for i in list(zip(latitudes,longitudes,probability))]\n",
    "\n",
    "    # Create a base map with satellite tiles\n",
    "    m = folium.Map(location=[sum(latitudes)/len(latitudes), sum(longitudes)/len(longitudes)], \n",
    "                   zoom_start=14,\n",
    "                   max_zoom=15,\n",
    "                   tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
    "                   attr='Google Satellite',\n",
    "                   width='100%',\n",
    "                   height = '100%')\n",
    "    \n",
    "    # plot heatmap on the map\n",
    "    plugins.HeatMap(heatdata, radius=20, blur=20, min_opacity=0.0).add_to(m)\n",
    "    \n",
    "    \"\"\"# #print each point in the map\n",
    "    for point in point_grid:\n",
    "        folium.Marker(location=point,popup=str(point)).add_to(m)\"\"\"\n",
    "    \n",
    "    # add farm image on the mapp\n",
    "    try:\n",
    "        if(image_path != \" \"):\n",
    "\n",
    "            overlay =  ImageOverlay(\n",
    "                image_path,\n",
    "                bounds= [[min(latitudes),min(longitudes)],[max(latitudes),max(longitudes)]],\n",
    "                opacity = 0.5\n",
    "            )\n",
    "\n",
    "            overlay.add_to(m)\n",
    "    except:\n",
    "        pass\n",
    "        ##print(\"Please provide a valid image path\")\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "def final_heatmap_with_image_locations(lat_boundaries, long_boundaries, dataset, image_path=\" \"):\n",
    "    \"\"\"This function returns the final heatmap with image taken locations with markers\"\"\"\n",
    "    location, frame_count = location_grid_frame_count(\"./images/images/\")\n",
    "    \n",
    "    #applying masking\n",
    "    dataset = make_mask_dataset(lat_boundaries,long_boundaries,dataset)\n",
    "    if dataset.empty == True:\n",
    "        m = np.NaN\n",
    "        return m\n",
    "    \n",
    "    longitudes = dataset[\"longitude\"]\n",
    "    latitudes = dataset[\"latitude\"]\n",
    "    probability = dataset[\"final_prob\"]\n",
    "\n",
    "    # heat map data ([lat,long,prob])\n",
    "    heatdata = [list(i) for i in list(zip(latitudes,longitudes,probability))]\n",
    "\n",
    "    # Create a base map with satellite tiles\n",
    "    m = folium.Map(location=[sum(latitudes)/len(latitudes), sum(longitudes)/len(longitudes)], \n",
    "                   zoom_start=14,\n",
    "                   max_zoom=15,\n",
    "                   tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
    "                   attr='Google Satellite',\n",
    "                   width='100%',\n",
    "                   height = '100%')\n",
    "    \n",
    "    # plot heatmap on the map\n",
    "    plugins.HeatMap(heatdata, radius=20, blur=20, min_opacity=0.0).add_to(m)\n",
    "\n",
    "    # Add markers for each coordinate in the list\n",
    "    for i, coord in enumerate(location):\n",
    "        folium.Marker(location=coord, popup=f'{frame_count[i]}').add_to(m)\n",
    "        \n",
    "    \"\"\"# #print each point in the map\n",
    "    for point in point_grid:\n",
    "        folium.Marker(location=point,popup=str(point)).add_to(m)\"\"\"\n",
    "    \n",
    "    # add farm image on the mapp\n",
    "    try:\n",
    "        if(image_path != \" \"):\n",
    "\n",
    "            overlay =  ImageOverlay(\n",
    "                image_path,\n",
    "                bounds= [[min(latitudes),min(longitudes)],[max(latitudes),max(longitudes)]],\n",
    "                opacity = 0.5\n",
    "            )\n",
    "\n",
    "            overlay.add_to(m)\n",
    "    except:\n",
    "        pass\n",
    "        ##print(\"Please provide a valid image path\")\n",
    "\n",
    "    return m\n",
    "\n",
    "def final_heatmap(lat_boundaries, long_boundaries, dataset, image_path=\" \"):\n",
    "    \n",
    "    \"\"\"This function returns the final heatmap\"\"\"\n",
    "    \n",
    "    #applying masking\n",
    "    dataset = make_mask_dataset(lat_boundaries,long_boundaries,dataset)\n",
    "    if dataset.empty == True:\n",
    "        m = np.NaN\n",
    "        return m\n",
    "    \n",
    "    longitudes = dataset[\"longitude\"]\n",
    "    latitudes = dataset[\"latitude\"]\n",
    "    probability = dataset[\"final_prob\"]\n",
    "\n",
    "    # heat map data ([lat,long,prob])\n",
    "    heatdata = [list(i) for i in list(zip(latitudes,longitudes,probability))]\n",
    "\n",
    "    # Create a base map with satellite tiles\n",
    "    m = folium.Map(location=[sum(latitudes)/len(latitudes), sum(longitudes)/len(longitudes)], \n",
    "                   zoom_start=14,\n",
    "                   max_zoom=15,\n",
    "                   tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
    "                   attr='Google Satellite',\n",
    "                   width='100%',\n",
    "                   height = '100%')\n",
    "    \n",
    "    # plot heatmap on the map\n",
    "    plugins.HeatMap(heatdata, radius=20, blur=20, min_opacity=0.0).add_to(m)\n",
    "     \n",
    "    \"\"\"# #print each point in the map\n",
    "    for point in point_grid:\n",
    "        folium.Marker(location=point,popup=str(point)).add_to(m)\"\"\"\n",
    "    \n",
    "    # add farm image on the mapp\n",
    "    try:\n",
    "        if(image_path != \" \"):\n",
    "\n",
    "            overlay =  ImageOverlay(\n",
    "                image_path,\n",
    "                bounds= [[min(latitudes),min(longitudes)],[max(latitudes),max(longitudes)]],\n",
    "                opacity = 0.5\n",
    "            )\n",
    "\n",
    "            overlay.add_to(m)\n",
    "    except:\n",
    "        pass\n",
    "        ##print(\"Please provide a valid image path\")\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd4d1f8",
   "metadata": {},
   "source": [
    "## Final calling Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cb8fe039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_maps_api(lat_boundaries,long_boundaries):    \n",
    "    \"\"\"This is the final fuction that we need to call when we are using api when we use user defined boundaries\"\"\"\n",
    "    \n",
    "    box_lat_list, box_long_list = find_box(lat_boundaries,long_boundaries)\n",
    "    \n",
    "    # extract the data from user inputs    \n",
    "    start_lat = max(box_lat_list)\n",
    "    end_lat = min(box_lat_list)\n",
    "    start_long = min(box_long_list)\n",
    "    end_long = max(box_long_list)\n",
    "\n",
    "    lat, long, cols, raws = api_to_latlong(lat_boundaries,long_boundaries)\n",
    "    dataset = download_weather_data_old(lat,long,cols, raws)\n",
    "    dataset = add_date_time(dataset)\n",
    "    dataset = final_probability(dataset,lat,long)\n",
    "       \n",
    "    spatial_map = spatial_heatmap(lat,long,lat_boundaries, long_boundaries)  \n",
    "    final_map = final_heatmap(lat_boundaries, long_boundaries,dataset)\n",
    "    create_mysql_table(dataset, FINAL_WEATHER_TABLE)\n",
    "    \n",
    "    spatial_map.save(SPATIAL_MAP_SAVE_PATH)\n",
    "    final_map.save(FINAL_MAP_SAVE_PATH)\n",
    "\n",
    "    with open(SPATIAL_MAP_SAVE_PATH, 'r', encoding='utf-8') as file_sp:\n",
    "        spatial_html_content = file_sp.read()\n",
    "        \n",
    "    with open(FINAL_MAP_SAVE_PATH, 'r', encoding='utf-8') as file_fn:\n",
    "        finalmap_html_content = file_fn.read()\n",
    "\n",
    "    return spatial_html_content,finalmap_html_content\n",
    "\n",
    "\n",
    "def final_maps_api_parallel(lat_boundaries,long_boundaries,api_keys):\n",
    "\n",
    "    \"\"\"This is the final fuction that we need to call when we are using api when we use user defined boundaries and parallel downloading\"\"\"\n",
    " \n",
    "    dataset,lat,long,cols,raws = create_weather_dataset(lat_boundaries,long_boundaries,api_keys)\n",
    "    dataset = add_date_time(dataset)\n",
    "    dataset = final_probability(dataset,lat,long)\n",
    "       \n",
    "    spatial_map = spatial_heatmap(lat,long,lat_boundaries, long_boundaries)  \n",
    "    final_map = final_heatmap(lat_boundaries, long_boundaries,dataset)\n",
    "    create_mysql_table(dataset, FINAL_WEATHER_TABLE)\n",
    "\n",
    "    spatial_map.save(SPATIAL_MAP_SAVE_PATH)\n",
    "    final_map.save(FINAL_MAP_SAVE_PATH)\n",
    "\n",
    "    with open(SPATIAL_MAP_SAVE_PATH, 'r', encoding='utf-8') as file_sp:\n",
    "        spatial_html_content = file_sp.read()\n",
    "        \n",
    "    with open(FINAL_MAP_SAVE_PATH, 'r', encoding='utf-8') as file_fn:\n",
    "        finalmap_html_content = file_fn.read()\n",
    "\n",
    "    return spatial_html_content,finalmap_html_content\n",
    "\n",
    "\n",
    "def final_maps(lat_boundaries,long_boundaries,api_keys):\n",
    "    \n",
    "    \"\"\"This is the final fuction that we need to call when we are using this notebook\"\"\"\n",
    "\n",
    "    time= temporal_processing_time(lat_boundaries,long_boundaries) + spatial_processing_time(lat_boundaries,long_boundaries)\n",
    "    print(f\"This will take {time} mins to complete\")\n",
    "    dataset,lat, long,cols,raws = create_weather_dataset(lat_boundaries,long_boundaries,api_keys)\n",
    "    dataset = add_date_time(dataset)\n",
    "    dataset = final_probability(dataset,lat,long)\n",
    "    \n",
    "    spatial_map = spatial_heatmap(lat,long,lat_boundaries, long_boundaries)  \n",
    "    final_map = final_heatmap(lat_boundaries, long_boundaries,dataset)\n",
    "    create_mysql_table(dataset, FINAL_WEATHER_TABLE)\n",
    "\n",
    "    spatial_map.save(SPATIAL_MAP_SAVE_PATH)\n",
    "    final_map.save(FINAL_MAP_SAVE_PATH)\n",
    "    \n",
    "    return spatial_map,final_map,dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a015e25",
   "metadata": {},
   "source": [
    "### UI part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "67efd216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_a = [-35.083200762,-35.142200762,143.251973043,143.316973043]\n",
    "\n",
    "api_keys = [\"8ee842d65cf08ec205365865e3d53348\", \"c29f27459329c5bbffb6e633e0fc4502\", \"51fb82fd74e9c378a1983d2551733418\", \"94979cc6f8c54c197d859f25576fb942\", \"21492bb5c90d0b7156cb1c5c543cb3c2\"]\n",
    "\n",
    "lat_boundaries = [-35.08491940916005,-35.11377165513988, -35.11377165513988, -35.128093335964095, -35.12816353408006, -35.14255287012573,-35.14248268441524, -35.12876021562264, -35.11377165513988, -35.11363123404099, -35.098464331083484, -35.08475718058806, -35.08478375410176, -35.08486825609331]\n",
    "\n",
    "long_boundaries = [143.2558918258238, 143.2560585113483, 143.28189354857975, 143.28193646392398, 143.29060536346012, 143.29051953277164, 143.3088443847614, 143.3087585540729, 143.30901604613834, 143.31601124724918, 143.31236344298893, 143.3107755752521, 143.25589646028465, 143.25589109586662]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "14992a32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if __name__ == \"__main__\":\\n    spatial_map,final_map = final_maps_api_parallel(lat_boundaries,long_boundaries,api_keys)'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"if __name__ == \"__main__\":\n",
    "    spatial_map,final_map = final_maps_api_parallel(lat_boundaries,long_boundaries,api_keys)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4491f275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "35b86bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAPI key for the python programm:\\nhttp://127.0.0.1:5000/pollination/?lat_boundaries=-35.08491940916005,-35.11377165513988,-35.11377165513988,-35.128093335964095,-35.12816353408006,-35.14255287012573,-35.14248268441524,-35.12876021562264,-35.11377165513988,-35.11363123404099,-35.098464331083484,-35.08475718058806,-35.08478375410176,-35.08486825609331&long_boundaries=143.2558918258238,143.2560585113483,143.28189354857975,143.28193646392398,143.29060536346012,143.29051953277164,143.3088443847614,143.3087585540729,143.30901604613834,143.31601124724918,143.31236344298893,143.3107755752521,143.25589646028465,143.25589109586662\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "API key for the python programm:\n",
    "http://127.0.0.1:5000/pollination/?lat_boundaries=-35.08491940916005,-35.11377165513988,-35.11377165513988,-35.128093335964095,-35.12816353408006,-35.14255287012573,-35.14248268441524,-35.12876021562264,-35.11377165513988,-35.11363123404099,-35.098464331083484,-35.08475718058806,-35.08478375410176,-35.08486825609331&long_boundaries=143.2558918258238,143.2560585113483,143.28189354857975,143.28193646392398,143.29060536346012,143.29051953277164,143.3088443847614,143.3087585540729,143.30901604613834,143.31601124724918,143.31236344298893,143.3107755752521,143.25589646028465,143.25589109586662\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
